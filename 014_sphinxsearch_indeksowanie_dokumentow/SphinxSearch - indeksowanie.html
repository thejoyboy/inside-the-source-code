<p align="justify">Funkcjonowanie silników wyszukiwania pełnotekstowego (np. <i><b>SphinxSearch</b></i>) opiera się na dwóch specjalizowanych procesach: indeksującym dokumenty (<i>indekser</i>) oraz umożliwiającym wyszukiwanie. Jednak do prawidłowego działania wyszukiwania niezbędny jest indeks będący efektem procesu <i><b>indeksowania</b></i>. Dane w takim indeksie zorganizowane są w postaci tzw. <i><b>indeksu odwróconego</b></i> czyli struktury danych zawierającej identyfikatory dokumentów oraz pozycję, na której dany token się znajduje. Przechowywanie danych w ten sposób umożliwia bardzo szybkie wyszukiwanie - znając szukaną frazę / tokeny od razu mamy przyporządkowane dokumenty zawierające te wyrażenia.</p>

<p>Przykładowo mamy dokumenty zawierające:<br />

<ul>
<li>doc_id=1 : <i>The quick brown fox jumped over the lazy dog</i></li>
<li>doc_id=2 : <i>Quick &lt;i&gt;brown&lt;/i&gt; foxes leap over lazy &lt;b&gt;dogs&lt;/b&gt; in summer</i></li>
</ul>
 
które zostaną odwzorowane w indeksie w następujący sposób:</br /><br />

<div align="center">
<table border="0">
<tr><td>The</td><td>{doc_id=1, pos=1}</td><td></td></tr>
<tr><td>quick</td><td>{doc_id=1, pos=2}</td><td></td></tr>
<tr><td>brown</td><td>{doc_id=1, pos=3}</td><td>{doc_id=2, pos=2}
</td></tr>
<tr><td>fox</td><td>{doc_id=1, pos=4}</td><td></td></tr>
<tr><td>jumped</td><td>{doc_id=1, pos=5}</td><td></td></tr>
<tr><td>over</td><td>{doc_id=1, pos=6}</td><td>{doc_id=2, pos=5}</td></tr>
<tr><td>the</td><td>{doc_id=1, pos=7}</td><td></td></tr>
<tr><td>lazy</td><td>{doc_id=1, pos=8}</td><td>{doc_id=2, pos=6}</td></tr>
<tr><td>dog</td><td>{doc_id=1, pos=9}</td><td></td></tr>
<tr><td>Quick</td><td>{doc_id=2, pos=1}</td><td></td></tr>
<tr><td>foxes</td><td>{doc_id=2, pos=3}</td><td></td></tr>
<tr><td>leap</td><td>{doc_id=2, pos=4}</td><td></td></tr>
<tr><td>dogs</td><td>{doc_id=2, pos=7}</td><td></td></tr>
<tr><td>in</td><td>{doc_id=2, pos=8}</td><td></td></tr>
<tr><td>summer</td><td>{doc_id=2, pos=9}</td><td></td></tr>
</table>
</div>
</p>

<p align="justify">Wyrażenia w lewej kolumnie to <i><b>tokeny</b></i>, czyli pojedyncze jednostki lingwistyczne (np. słowa, liczby, daty, kwoty, adresy IP, adresy email itd.), które mogą zostać wyodrębnione z tekstu źródłowego. W dużym skrócie indeksowanie sprowadza się właśnie do wyodrębniania oraz przetwarzania wspomnianych tokenów, tak aby na tej podstawie można było zbudować <i>inverted index</i>. Proces ten (<i>indeksowanie</i>) jest bardzo złożony - nie sprowadza się wyłącznie do <i>tokenizacji</i> (rozbicia na tokeny), ale także usuwane są tagi HTMLowe, <i>stopwords</i>, tokeny sprowadzane są do postaci bazowej itd. Bardzo dobrze ilustruje to następujący schemat:<br /><br />

<img src="http://sphinxsearch.com/blog/wp-content/uploads/2014/11/5extended.png" /><br /><i>(źródło: sphinxsearch.com)</i><br />
</p>

<p>Poszczególne kroki przedstawione na tym schemacie zostały opisane na <a href="http://sphinxsearch.com/blog/2014/11/26/sphinx-text-processing-pipeline/" target="_blank">blogu Sphinxa</a>.</p>

<p align="justify">Wracając do przytoczonego powyżej odwzorowania tekstów (z przykładowych dokumentów) na tokeny w odwróconym indeksie, w kolejnych etapach indeksowania, po usunięciu tagów HTMLowych i wyodrębnieniu tokenów, ich lista zostanie ograniczona, np. zamiana wielkich liter na małe, usuwanie stopwords, sprowadzanie form w liczbie mnogiej do pojedynczej (np. <i>foxes</i> - <i>fox</i>), ewentualne synonimy (np. <i>leap</i> - <i>jump</i>) itd. Zatem, finalnie nasz indeks odwrócony będzie postaci:<br />

<div align="center">
<table border="0">
<tr><td>brown</td><td>{doc_id=1, pos=3}</td><td>{doc_id=2, pos=2}</td></tr>
<tr><td>dog</td><td>{doc_id=1, pos=9}</td><td>{doc_id=2, pos=7}</td></tr>
<tr><td>fox</td><td>{doc_id=1, pos=4}</td><td>{doc_id=2, pos=3}</td></tr>
<tr><td>in</td><td></td><td>{doc_id=2, pos=8}</td></tr>
<tr><td>jump</td><td>{doc_id=1, pos=5}</td><td>{doc_id=2, pos=4}</td></tr>
<tr><td>lazy</td><td>{doc_id=1, pos=8}</td><td>{doc_id=2, pos=6}</td></tr>
<tr><td>over</td><td>{doc_id=1, pos=6}</td><td>{doc_id=2, pos=5}</td></tr>
<tr><td>quick</td><td>{doc_id=1, pos=2}</td><td>{doc_id=2, pos=1}</td></tr>
<tr><td>summer</td><td></td><td>{doc_id=2, pos=9}</td></tr>
</table>
</div>
</p>

<p>Uwzględniając wszystkie te etapy procesu indeksowania przez <i>SphinxSearch</i>, przykładowa konfiguracja wygląda następująco:<br /><br />

<code data-gist-id="b4794ae38b2ad7ced9d2" data-gist-line="205-216" data-gist-highlight-line="211-214" data-gist-hide-footer="true" data-gist-hide-line-numbers="true"></code>
</p>

<p>Proponowana konfiguracja zawiera takie elementy jak <i>stopwords</i>, <i>wordforms</i> oraz <i>morphology</i>.</p>

<p align="justify"><i><b>Stopwords</b></i> to wyrażenia nieistotne, nie mające żadnej albo niewielką wartość informacyjną podczas wyszukiwania. Przykładami mogą być <i>the</i>, <i>a</i>, <i>an</i> dla języka angielskiego. Wyrażenia takie nie zostaną zaindeksowane, natomiast podczas wyszukiwania zostaną pominięte w wyszukiwanej frazie.<p/> 

<p align="justify">Pod pojęciem <i><b>wordforms</b></i>, w przypadku SphinxSearch, należy rozumieć pewnego rodzaju słownik zawierający przekształcenia jednych wyrażeń na drugie. W ten sposób możemy dostarczyć dla <i>indeksera</i> słownik synonimów, np. <i>leap</i> - <i>jump</i>, <i>s02e01</i> - <i>season 2 episode 1</i> itd. Indeksując wyrażenie, dla którego definiujemy inną formę (np. synonim) będzie ono reprezentowane (w indeksie) jako ta nowa forma, szukając - słowa z szukanej frazy także zostaną zamienione na zmapowaną formę wyrażenia.</p>

<p align="justify"><i><b>Morphology</b></i> definiuje listę użytych preprocesorów które zostaną wykorzystane do przetworzenia tokenów, do których możemy zaliczyć m.in <i><b>lematyzery</b></i> i <i><b>stemmery</b></i>. Więcej informacji odnośnie wykorzystania lemmatyzerów i stemmerów możecie znaleźć w jednym z moich poprzednich <a href="http://inside-the-source-code.blogspot.com/2014/06/sphinxsearch-odmiana-wyrazen.html" target="_blank">artykułów</a>
oraz na oficjalnym <a href="sphinxsearch.com/blog/2013/07/15/morphology-processing-with-sphinx/" target="_blank">blogu</a> SphinxSearch. Dzięki tej opcji możemy uzyskać sprowadzenie tokenów w liczbie mnogiej do pojedynczej czy też ich do formy bazowej, np. <i>foxes</i> - <i>fox</i>, <i>jumped</i> - <i>jump</i> itd. SphinxSearch jest pod tym względem (lematyzacja, stemming) cały czas rozwijany i sukcesywnie dodawane są kolejne preprocesory. Wykorzystany w załączonym przykładzie stemmer (stem_en) to tzw. <i>Porter's English stemmer</i>. Niestety nie jest to rozwiązanie idealne i zdarzają się nie zawsze poprawne przekształcenia, np. <i>lazy</i> - <i>lazi</i>. Niedociągnięcia takie możemy korygować za pomocą wordforms, z tym że należy mieć na uwadze kolejność etapów podczas indeksowania. Jeśli spojrzymy na przedstawiony schemat z kolejnymi krokami, zauważymy że wordforms są wykonywane przed morphology. Możemy jednak skorzystać z faktu, iż etap <i>Post-Morphology Wordforms</i> wykonywany jest po <i>morphology</i> - do pliku zawierającego <i>wordforms</i> dodajemy przekształcenie interesującego nas wyrażenia poprzedzając oryginalne wyrażenie znakim ~, np. <i>~lazy > lazy</i>. W ten sposób osiągniemy nasz cel, czyli skorygujemy nieprawidłowe przekształcenia stemmera. Ostatecznie, zaindeksowane zostanie słowo <i>lazy</i>, nawet pomimo przekształcenia go na <i>lazi</i> przez stemmer Portera. Alternatywą dla użycia stemmera Portera oraz dodatkowej korekty za pomocą wordforms może być wykorzystanie lematyzera języka angielskiego - tam nie występują <i>dziwne</i> formy bazowe wyrażeń. Efekt końcowy będzie identyczny, tyle że sam proces indeksowania będzie nieco prostszy.</p>

<p>Alternatywna konfiguracja:<br /><br />

<code data-gist-id="b4794ae38b2ad7ced9d2" data-gist-line="216-225" data-gist-highlight-line="222" data-gist-hide-footer="true" data-gist-hide-line-numbers="true"></code>
</p>

<p>Zweryfikujmy zatem jak ostatecznie wygląda po stronie naszego silnika wyszukiwawczego:<br /><br />
 
<code data-gist-id="462c21663745e7280f78" data-gist-line="1-13" data-gist-hide-footer="true" data-gist-hide-line-numbers="true"></code>

<br />

<code data-gist-id="462c21663745e7280f78" data-gist-line="15-29" data-gist-hide-footer="true" data-gist-hide-line-numbers="true"></code>

</p>

<p align="justify">Chciałbym zwrócić jeszcze uwagę na inną przydatną opcję podczas indeksowania i wyszukiwania. Domyślnie wszystkie znaki nie uwzględnione w <i><b>charset_table</b></i> traktowane są jako separatory. Może jednak pojawić się potrzeba wyszukiwania tokenów zawierających specyficzne znaki, jednocześnie traktując je jako separatory, np. frazę <i>user@sphinxsearch.com</i> chcielibyśmy rozbić na tokeny <i>user</i>, <i>sphinxsearch.com</i> oraz <i>user@sphinxsearch.com</i>. W takiej sytuacji skorzystamy z opcji <i><b>blend_chars</b></i> zawierającej znaki, które będą traktowane jako separatory oraz jednocześnie jako prawidłowe znaki. W przedstawionym przykładzie konfiguracja indeksu będzie wyglądała następująco:<br /><br />

<code data-gist-id="b4794ae38b2ad7ced9d2" data-gist-line="225-235" data-gist-highlight-line="232-233" data-gist-hide-footer="true" data-gist-hide-line-numbers="true"></code>

</p>

<p align="justify">Dodatkowo istnieje możliwość skonfigurowania w jaki sposób wyrażenie zawierające znaki określone jako <i>blend_chars</i> zostanie zaindeksowane (parametr konfiguracji <i><b>blend_mode</b></i>). Domyślnie zaindeksowany zostanie cały token, ale czasami będziemy potrzebowali innej reprezentacji. Przykładowo wyrażenie <i>@sphinxsearch!</i> będziemy potrzebowali w formie tokenów: <i>@sphinxsearch</i>, <i>sphinxsearch!</i>, <i>@sphinxsearch!</i> oraz <i>sphinxsearch</i>. Polegając na domyślnej konfiguracji uzyskamy wyłącznie tokeny <i>@sphinxsearch!</i> oraz <i>sphinxsearch</i>. Ustawiając tryby <i><b>trim_head</b></i>, <i><b>trim_tail</b></i> oraz <i><b>trim_none</b></i> osiągniemy postać jakiej oczekiwaliśmy.<br /><br />

<code data-gist-id="462c21663745e7280f78" data-gist-line="31-46" data-gist-hide-footer="true" data-gist-hide-line-numbers="true"></code>
</p>

<p>Pliki indeksu zapisywane są przez <i>SphinxSearch</i> w kilku plikach o takiej nazwie jak nazwa indeksu, ale posiadających różne rozszerzenia. Każdy z nich pełni osobną funkcję:<br />

<ul> 
<li><i>.spa</i> - przechowuje wartości atrybutów, nie podlegają procesowi <i>tokenizacji</i> jak to ma miejsce w przypadku pól tekstowych (ang. <i>fields)</i></li>
<li><i>.spd</i> - identyfikatory dokumentów (ang. <i>doclist</i>) przyporządkowanych do poszczególnych tokenów</li>
<li><i>.sph</i> - nagłówek indeksu zawierający m.in typy pól oraz atrybutów, ścieżka do pliku zawierającym stopwords itd.</li>
<li><i>.spi</i> - słownik zawierający wyodrębnione (z pól tekstowych) tokeny (ang. <i>wordlist</i>)</li>
<li><i>.spk</i> - kill list, czyli identyfikatory dokumentów wykluczonych z danych indeksów (przydatne podczas obsługi operacji typu <i>delete</i> / <i>update</i> bez konieczności przeindeksowania całości)</li>
<li><i>.spl</i> - locki zakładane na indeksy (ang. <i>locks</i>)</li>
<li><i>.spm</i> - wartości typu MVA (ang. <i>multi-valued attributes</i>)</li>
<li><i>.spp</i> - pozycja danego tokenu w indeksowanym tekście (ang. <i>hitlist</i>)</li>
<li><i>.sps</i> - atrybuty tekstowe</li>
</ul>
</p>

<p>Szczegółowy opis poszczególnych elementów indeksu znajdziecie <a href="http://sphinxsearch.googlecode.com/svn/trunk/doc/internals-index-format.txt" target="_blank">tutaj</a>.</p>

<p align="justify">Ponadto przydatne może okazać się narzędzie (<i>indextool</i>) dostępne w pakiecie <i>SphinxSearch</i> pozwalające podejrzeć konfigurację, nagłówek czy też inne elementy indeksu. Najważniejsze opcje:<br />

<ul>
<li><i>--dumpconfig {index_name}.sph</i> - podgląd konfiguracji indeksu</li>
<li><i>--dumpheader {index_name}.sph</i> - podgląd nagłówka indeksu</li>
<li><i>--dumpdocids {index_name}</i> - lista identyfikatorów dokumentów w indeksie</li>
<li><i>--dumphitlist {index_name} {keyword}</i> - lista identyfikatorów dokumentów wraz pozycją, w których dane słowo występuje</li>
<li><i>--fold {index_name} {path_to_file}</i> - weryfikacja jak wyrażenie z podanego pliku zostanie rozbite na tokeny</li>
</ul>
</p>

<p>Alternatywą dla ostatniej z wymienionych powyżej opcji może być użycie <i><b>CALL KEYWORDS({keywords}, {index_name});</b></i> z poziomu SphinxQL.</p>

<p align="justify"><b>Indeksowanie</b> dokumentów przez <i>SphinxSearch</i> jest wieloetapowym złożonym procesem. Wiedząc jakie operacje wykonywane są w danym kroku, możemy w pełni świadomy i wygodny dla nas sposób konfigurować indeks, tak aby odpowiadał naszym potrzebom. Dodatkowo dzięki dostępnym narzędziom będziemy w stanie przeanalizować na jakie tokeny rozbijany będzie indeksowany tekst, co znacznie ułatwia weryfikację czy dany dokument będzie pasował do szukanej frazy. W końcu, <i>indextool</i> może zostać wykorzystany do wyciągania informacji (konfiguracja, nagłówek, identyfikatory dokumentów) z plików indeksów. Zatem, udanego 
indeksowania!</p>

<p>Przydatne linki:<br />

<ul>
<li><a href="http://en.wikipedia.org/wiki/Inverted_index " target="_blank">http://en.wikipedia.org/wiki/Inverted_index </a></li>
<li><a href="http://rosettacode.org/wiki/Inverted_Index" target="_blank">http://rosettacode.org/wiki/Inverted_Index</a></li>
<li><a href="http://en.wikipedia.org/wiki/Search_engine_indexing" target="_blank">http://en.wikipedia.org/wiki/Search_engine_indexing</a></li>
<li><a href="http://www.ir.uwaterloo.ca/book/" target="_blank">http://www.ir.uwaterloo.ca/book/</a></li>
<li><a href="https://www.thunderstone.com/site/texisman/tokenization_and_inverted.html" target="_blank">https://www.thunderstone.com/site/texisman/tokenization_and_inverted.html</a></li>
<li><a href="http://www.cs.sfu.ca/CourseCentral/456/jpei/web%20slides/L07%20-%20Tokenization.pdf" target="_blank">http://www.cs.sfu.ca/CourseCentral/456/jpei/web%20slides/L07-Tokenization.pdf</a></li>
<li><a href="http://sphinxsearch.com/blog/2014/11/26/sphinx-text-processing-pipeline/" target="_blank">http://sphinxsearch.com/blog/2014/11/26/sphinx-text-processing-pipeline/</a></li>
<li><a href="http://www.ivinco.com/blog/sphinx-in-action-how-sphinx-handles-text-during-indexing/" target="_blank">http://www.ivinco.com/blog/sphinx-in-action-how-sphinx-handles-text-during-indexing/</a></li>
<li><a href="http://www.nearby.org.uk/sphinx/sphinx-tokenizing.gif" target="_blank">http://www.nearby.org.uk/sphinx/sphinx-tokenizing.gif</a></li>
<li><a href="http://sphinxsearch.com/blog/2014/12/04/how-to-use-the-wordforms-list/" target="_blank">http://sphinxsearch.com/blog/2014/12/04/how-to-use-the-wordforms-list/</a></li>
<li><a href="http://sphinxsearch.com/blog/2013/07/15/morphology-processing-with-sphinx/" target="_blank">http://sphinxsearch.com/blog/2013/07/15/morphology-processing-with-sphinx/</a></li>
<li><a href="http://sphinxsearch.com/blog/2013/12/05/working-with-the-english-lemmatizer/" target="_blank">http://sphinxsearch.com/blog/2013/12/05/working-with-the-english-lemmatizer/</a></li>
<li><a href="http://search.blox.pl/2010/01/Myslec-po-polsku.html" target="_blank">http://search.blox.pl/2010/01/Myslec-po-polsku.html</a></li>
<li><a href="http://sphinxsearch.com/blog/2012/08/14/indexing-tips-tricks/" target="_blank">http://sphinxsearch.com/blog/2012/08/14/indexing-tips-tricks/</a></li>
<li><a href="http://www.ivinco.com/blog/how-to-improve-sphinx-indexing-performance/" target="_blank">http://www.ivinco.com/blog/how-to-improve-sphinx-indexing-performance/</a></li>
<li><a href="http://blog.stunf.com/building-a-scalable-real-time-search-architecture-with-sphinx/" target="_blank">http://blog.stunf.com/building-a-scalable-real-time-search-architecture-with-sphinx/</a></li>
<li><a href="http://sphinxsearch.googlecode.com/svn/trunk/doc/internals-index-format.txt" target="_blank">http://sphinxsearch.googlecode.com/svn/trunk/doc/internals-index-format.txt</a></li>
<li><a href="http://sphinxsearch.com/docs/current.html#confgroup-index" target="_blank">http://sphinxsearch.com/docs/current.html#confgroup-index</a></li>
</ul>
</p>



